A high-performance 7-class Speech Emotion Recognition system trained on the RAVDESS dataset achieving 86.9% accuracy and 0.866 F1-score. This implementation addresses class imbalance, overfitting, and the notorious "neutral class problem" that plagues most SER systems.

ğŸš€ Features
7 Emotion Classes: Neutral, Happy, Sad, Angry, Fear, Disgust, Surprise
Balanced Dataset: Equal samples per class to prevent bias
Data Augmentation: Pitch shifting, time stretching, and noise injection
Class Weighting: Handles inherent class imbalance in emotion datasets
Lightweight Architecture: Only 1.5M parameters for fast inference
Production Ready: Complete training, evaluation, and inference pipeline
Kaggle/Colab Compatible: Ready to run in cloud environments
ğŸ“Š Performance Metrics
Neutral
0.60
0.50
0.55
12
Happy
1.00
0.92
0.96
12
Sad
0.92
1.00
0.96
12
Angry
0.75
1.00
0.86
12
Fear
0.92
0.92
0.92
12
Disgust
0.91
0.83
0.87
12
Surprise
1.00
0.92
0.96
12
OVERALL
0.87
0.87
0.87
84
Note: Neutral class performance (55% F1) is considered excellent in SER research, where 40-60% is typical. 

ğŸ“¦ Installation
Prerequisites
Python 3.8+
PyTorch 2.0+
CUDA-compatible GPU (optional but recommended)
Setup
bash


1
2
3
4
5
6
7
8
# Clone the repository
git clone https://github.com/yourusername/SpeechEmotionRecognition-Ravdess.git
cd SpeechEmotionRecognition-Ravdess

# Install dependencies
pip install -r requirements.txt

# For Kaggle/Colab: The notebook automatically installs required packages
ğŸƒâ€â™‚ï¸ Quick Start
Training on RAVDESS Dataset
Download RAVDESS Dataset:
Go to RAVDESS on Kaggle
Add dataset to your Kaggle notebook or download locally
Run Training:
python


1
python speech_emotion_recognition.py
For Kaggle/Colab: Simply run the provided notebook cells
Single File Prediction
python


1
2
3
4
5
6
7
8
9
10
11
12
13
from speech_emotion_recognition import load_trained_model, predict_emotion
import torch

# Load pre-trained model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model, emotion_map, idx_to_emotion = load_trained_model('final_speech_emotion_model.pth')

# Predict emotion from audio file
audio_path = "path/to/your/audio.wav"
emotion, confidence = predict_emotion(model, audio_path, device)

print(f"Predicted Emotion: {emotion}")
print(f"Confidence: {confidence:.2f}")
ğŸ“ Project Structure


1
2
3
4
5
6
7
8
speech-emotion-recognition/
â”œâ”€â”€ speech_emotion_recognition.py    # Main implementation
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ final_speech_emotion_model.pth  # Trained model (after training)
â”œâ”€â”€ README.md                       # This file
â””â”€â”€ notebooks/                      # Example notebooks
    â”œâ”€â”€ ravdess_training.ipynb
    â””â”€â”€ inference_demo.ipynb
âš™ï¸ Technical Details
Model Architecture
Input: 13 MFCC features Ã— 94 time steps (3-second audio)
Backbone: 3-layer 1D CNN with batch normalization
Output: 7-class emotion classification
Parameters: ~1.5 million (lightweight and efficient)
Training Configuration
Dataset: RAVDESS (balanced sampling)
Batch Size: 16
Learning Rate: 1e-3
Epochs: 15-20
Optimizer: AdamW with weight decay
Loss: Weighted Cross-Entropy
Data Augmentation
Pitch Shift: Â±2 semitones
Time Stretch: 0.9x - 1.1x
Noise Injection: Gaussian noise (0.5% - 2% amplitude)
Gain Adjustment: Â±20% volume variation
ğŸ¯ Use Cases
Mental Health Applications: Emotion monitoring in therapy sessions
Customer Service: Real-time caller emotion detection
Human-Computer Interaction: Emotion-aware AI assistants
Entertainment: Emotion-responsive gaming and media
Research: Baseline model for speech emotion recognition studies
ğŸ“š References
RAVDESS Dataset: Livingstone, S. R., & Russo, F. A. (2018). The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS).
MFCC Features: Davis, S., & Mermelstein, P. (1980). Comparison of parametric representations for monosyllabic word recognition.
Speech Emotion Recognition: Schuller, B., Steidl, S., & Vinciarelli, A. (2009). The INTERSPEECH 2009 Emotion Challenge.
ğŸ¤ Contributing
Contributions are welcome! Please feel free to submit issues, feature requests, or pull requests.

Fork the repository
Create your feature branch (git checkout -b feature/AmazingFeature)
Commit your changes (git commit -m 'Add some AmazingFeature')
Push to the branch (git push origin feature/AmazingFeature)
Open a Pull Request
ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
RAVDESS dataset creators for providing high-quality emotional speech data
Librosa and PyTorch communities for excellent audio and deep learning libraries
Kaggle community for hosting the dataset and providing computational resources






